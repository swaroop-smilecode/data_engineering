#### Big data problem
`<Pending notes>`
#### Hadoop HDFS & Map Reduce framework
`<Pending notes>`
#### Apache Spark
- Apache Spark is a unified computing engine and a set of libraries for parallel data processing on computer clusters.
  ![image](https://github.com/user-attachments/assets/e1384646-6f81-4aeb-a60b-caa147010535)

- Before spark, hadoop was doing the samething. We know the evolution process</br>
  First, one technology comes up & as time progresses, disadvantage of that technology pop's up</br>
  To overcome that disadvantage, new technology evolves & that's how spark evolved.</br>
  So, what was the problem with hadoop?</br>
  Before analysing the data with hadoop's map reduce framework,</br>
  that data needs to be copied from source location to HDFS & moving terabytes of data is costly.

#### PySpark
Spark is written in `scala` language.</br>
PySpark is python language wrapper around spark.
